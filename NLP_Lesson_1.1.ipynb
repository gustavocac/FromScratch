{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copy of NLP Lesson 1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Fg7BRDIlng39",
        "Y0iUMdcqng4D",
        "FpQXkbflng4R",
        "T9z8q7i3ng4Y",
        "8cf2lGs4ng4b",
        "0xblKMB-ng4n"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gustavocac/FromScratch/blob/master/NLP_Lesson_1.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In2o9G5ing3s",
        "colab_type": "text"
      },
      "source": [
        "<h1><div align=\"center\">Processamento de Linguagem Natural</div></h1>\n",
        "<div align=\"center\">Gustavo C. A Corradi</div>\n",
        "<div align=\"center\"><a href=\"http://robotreport.me/\">robotreport.me</a></div>\n",
        "<div align=\"center\">@gustavocorradi</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0GCeR3Ing3s",
        "colab_type": "text"
      },
      "source": [
        "# Lição  I - Representação de Texto "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUoDt5Jmng3t",
        "colab_type": "text"
      },
      "source": [
        "Nesta lição, veremos em alguns detalhes como podemos representar melhor o texto em nosso aplicativo. Vamos começar importando os módulos que usaremos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07WKpt6cng3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "from pprint import pprint\n",
        "import gzip\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuvwHJ3wrSNj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e15f0771-4361-4e16-c51f-6fe27e1415a0"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"http://www.rb.org.br/detalhe_artigo.asp?id=2295&idioma=Portugues\"\n",
        "html = urllib.request.urlopen(url).read()\n",
        "soup = BeautifulSoup(html)\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()    # rip it out\n",
        "\n",
        "# get text\n",
        "text = soup.get_text()\n",
        "\n",
        "# break into lines and remove leading and trailing space on each\n",
        "#lines = (line.strip() for line in text.splitlines())\n",
        "# break multi-headlines into a line each\n",
        "#chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "# drop blank lines\n",
        "#text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "print(text)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Radiologia Brasileira - \r\n",
            "Contrastes orais neutros para enterografia por tomografia computadorizada\r\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Radiologia Brasileira - Publicação Científica Oficial do Colégio Brasileiro de Radiologia\n",
            "AMB - Associação Médica Brasileira CNA - Comissão Nacional de Acreditação\n",
            "\r\n",
            "      Idioma/Language:\r\n",
            "      \n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Home\n",
            "CBR\n",
            "RB History\n",
            "Editorial Board\n",
            "Publication Guidelines \n",
            "Advanced Search\n",
            "Current Issue\n",
            "Previous Issues\n",
            "Paper Submission\n",
            "Audience\n",
            "Supplements\n",
            "Ahead of Print\n",
            "Contact us\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\r\n",
            "          Vol. 45 nº 3 - Maio / Jun.  of 2012\r\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "EDITORIAL   \r\n",
            "      \n",
            "\n",
            " Print \n",
            "\n",
            "\n",
            "Page(s) V to VI\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\r\n",
            "\t\t\t\t\tContrastes orais neutros para enterografia por tomografia computadorizada\n",
            "\n",
            "\n",
            "\n",
            "Autho(rs): Daniella Braz Parente\n",
            "\n",
            "\n",
            "\n",
            " PDF Português      \n",
            " PDF English\n",
            "\n",
            "\n",
            "\n",
            "Texto em Português \n",
            "English Text \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Durante muitos anos o intestino delgado foi um órgão difícil de ser estudado. Até bem pouco tempo, os únicos métodos de imagem disponíveis para o estudo deste segmento eram o trânsito delgado e a enteróclise convencional, que apresentam diversas limitações(1,2).\n",
            "\n",
            "Nos últimos anos, com o desenvolvimento da tecnologia, vários métodos têm sido desenvolvidos para o estudo do intestino delgado. No campo da endoscopia, dispõe-se hoje da cápsula endoscópica(3) e da enteroscopia com duplo-balão(4), métodos caros e não amplamente disponíveis em nosso meio. No campo da radiologia, surgiram a enterografia e a enteróclise por tomografia computadorizada (TC) ou por ressonância magnética (RM), que combinam as vantagens dos métodos radiológicos convencionais com as dos métodos seccionais, ou seja, a distensão do lúmen intestinal associada à visualização multiplanar de toda a cavidade abdominal e pélvica, sem sobreposição de imagens. Com estes métodos, toda a espessura da parede intestinal é visualizada, incluindo mucosa, submucosa, muscular e serosa, assim como gordura perientérica, vasos retos, linfonodos e padrão de realce pelo contraste, o que permite identificar espessamentos parietais, sinais inflamatórios, fístulas, coleções, além de focos de sangramento e tumores, entre outros(2,57).\n",
            "\n",
            "A distensão luminal é um passo fundamental na interpretação do estudo do intestino delgado, já que alças colabadas podem esconder ou simular segmentos doentes(2,57). Neste número da Radiologia Brasileira, D'Ippolito et al.(8) avaliam o desempenho de diferentes meios de contraste orais neutros, comparando a capacidade de distensão intestinal, a definição da parede intestinal, a aceitação e os efeitos colaterais. Os autores descrevem o estudo de 30 pacientes submetidos a enterografia por TC, randomizados entre três tipos de contraste oral neutro disponíveis em nosso meio: água, leite integral e polietilenoglicol (PEG). Neste estudo, a concentração de PEG empregada foi inferior à utilizada em estudos anteriores, com o objetivo de reduzir seus efeitos indesejados, mas mantendo sua capacidade de distensão da luz intestinal. O preparo com PEG resultou na melhor distensão intestinal (57,5%), não havendo diferença significativa entre o leite (35%) e a água (25%). Em todos os grupos foi possível a definição adequada da parede intestinal e os meios de contraste tiveram boa aceitação. Dos pacientes que ingeriram PEG, 80% apresentaram diarreia, efeito colateral não observado com o uso de leite ou água. O estudo mostra, de forma clara, que água e leite não são bons meios de contraste oral para uso na enterografia, já que são absorvidos pelo intestino e não distendem adequadamente as alças, o que é muito importante para a correta interpretação do exame. Mostra também que o PEG é um bom meio de contraste oral para o estudo do intestino delgado e que os pacientes toleram bem a ingestão de grande volume de PEG (1.500 ml). Além disso, demonstra que a diarreia é um efeito indesejado habitual, que ocorre na maioria dos pacientes, inerente à propriedade do PEG de não ser absorvido ao longo do trato gastrintestinal, mas geralmente autolimitada. Estudos futuros comparando o uso de PEG em baixa concentração com PEG na concentração habitual são necessários para melhor avaliação da distensibilidade luminal e incidência de efeitos colaterais, principalmente diarreia. Também seria interessante a realização de estudos comparando o PEG com o manitol, já que este último tem um custo muito baixo e é amplamente disponível na rede hospitalar pública.\n",
            "\n",
            "A avaliação de pacientes com suspeita ou diagnóstico de doença de Crohn é uma das principais indicações de estudo do intestino delgado. São pacientes jovens, que necessitam de reavaliações durante toda a vida. Desta forma, deve ser levada em consideração a dose de radiação a que estes pacientes são submetidos e tentar sempre reduzir ao máximo a dose de radiação utilizada(9). Este aspecto foi contemplado no estudo de D'Ippolito et al.(8), no qual foi realizada apenas a fase enterográfica, já que as fases pré-contraste, arterial e tardia não acrescentam significativamente para o diagnóstico e quadriplicam a dose de radiação a que os pacientes são expostos. Outra opção para esta população é a enterografia por RM, que tem acurácia diagnóstica semelhante e a vantagem de não utilizar radiação, podendo ser repetida múltiplas vezes sem malefícios ao paciente(6).\n",
            "\n",
            "A comparação entre diferentes tipos de contraste oral disponíveis em nosso meio para uso na enterografia por TC é um tema bastante importante, visto a grande prevalência de pacientes com doenças no intestino delgado que podem se beneficiar deste método de imagem, como pacientes com doença inflamatória intestinal, notadamente os com doença de Crohn, pacientes com síndrome do cólon irritável que precisam excluir outras causas para a dor abdominal, os com sangramento intestinal e os com tumores do intestino delgado. A TC está amplamente disponível em nosso país e é importante que os radiologistas estejam familiarizados com este método, para contribuir de forma significativa no diagnóstico, tratamento e acompanhamento dos pacientes com doenças intestinais.\n",
            "\n",
            "\n",
            "REFERÊNCIAS\n",
            "\n",
            "1. Saibeni S, Rondonotti E, Iozzelli A, et al. Imaging of the small bowel in Crohn's disease: a review of old and new techniques. World J Gastroenterol. 2007;13:327987.\n",
            "\n",
            "2. Lee SS, Kim AY, Yang SK, et al. Crohn disease of the small bowel: comparison of CT enterography, MR enterography, and small-bowel follow-through as diagnostic techniques. Radiology. 2009;251:75161.\n",
            "\n",
            "3. Freitas GP, Teixeira N, Feldman G. Capsule endoscopy in clinical practice: four years of experience from a single center. Arq Gastroenterol. 2011;48:2202.\n",
            "\n",
            "4. Albert JG. Interventional balloon-enteroscopy. J Interv Gastroenterol. 2012;2:4250.\n",
            "\n",
            "5. Costa-Silva L, Martins T, Passos MCF. Enterografia por tomografia computadorizada: experiência inicial na avaliação das doenças do intestino delgado. Radiol Bras. 2010;43:3038.\n",
            "\n",
            "6. Siddiki HA, Fidler JL, Fletcher JG, et al. Prospective comparison of state-of-the-art MR enterography and CT enterography in small-bowel Crohn's disease. AJR Am J Roentgenol. 2009;193:11321.\n",
            "\n",
            "7. Masselli G, Gualdi G. Evaluation of small bowel tumors: MR entero­clysis. Abdom Imaging. 2010;35:2330.\n",
            "\n",
            "8. D'Ippolito G, Braga FA, Resende MC, et al. Enterografia por tomografia computadorizada: uma avaliação de diferentes contrastes orais neutros. Radiol Bras. 2012;45:13943.\n",
            "\n",
            "9. Desmond AN, O'Regan K, Curran C, et al. Crohn's disease: factors associated with exposure to high levels of diagnostic radiation. Gut. 2008;57:15249.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Doutoranda em Radiologia da Universidade Federal do Rio de Janeiro (UFRJ), Pesquisadora do Instituto D'Or de Pesquisa e Ensino, Radiologista da Rede Labs D'Or, Rio de Janeiro, RJ, Brasil. E-mail: daniella.parente@gmail.com\n",
            " Back  Top  Print \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "GN1© Copyright 2020 - All rights reserved to Colégio Brasileiro de Radiologia e Diagnóstico por ImagemAv. Paulista, 37 - 7° andar - Conj. 71 - CEP 01311-902 - São Paulo - SP - Brazil - Phone: (11) 3372-4544 - Fax: (11) 3372-4554 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQFKHU6Kng3w",
        "colab_type": "text"
      },
      "source": [
        "We choose a well known nursery rhyme, that has the added distinction of having been the first audio ever recorded, to be the short snippet of text that we will use in our examples:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYFMc4gZng3y",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "The first step in any analysis is to tokenize the text. What this means is that we will extract all the individual words in the text. For the sake of simplicity, we will assume that our text is well formed and that our words are delimited either by white space or punctuation characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P23u9wrLng3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_words(text):\n",
        "    temp = text.split() # Split the text on whitespace\n",
        "    text_words = []\n",
        "    print(temp)\n",
        "    for word in temp:\n",
        "        # Remove any punctuation characters present in the beginning of the word\n",
        "        print(word[0])\n",
        "        while word[0] in string.punctuation:\n",
        "            word = word[1:]\n",
        "\n",
        "        # Remove any punctuation characters present in the end of the word\n",
        "        while word[-1] in string.punctuation:\n",
        "            word = word[:-1]\n",
        "\n",
        "        # Append this word into our list of words.\n",
        "        text_words.append(word.lower())\n",
        "        \n",
        "    return text_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqEi4SP8ng30",
        "colab_type": "text"
      },
      "source": [
        "After this step we now have our text represented as an array of individual, lowercase, words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSvHGAfong30",
        "colab_type": "code",
        "outputId": "898339be-ce08-4e3c-f0b3-ce1026d17f99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text_words = extract_words(text)\n",
        "print(text_words)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Radiologia', 'Brasileira', '-', 'Contrastes', 'orais', 'neutros', 'para', 'enterografia', 'por', 'tomografia', 'computadorizada', 'Radiologia', 'Brasileira', '-', 'Publicação', 'Científica', 'Oficial', 'do', 'Colégio', 'Brasileiro', 'de', 'Radiologia', 'AMB', '-', 'Associação', 'Médica', 'Brasileira', 'CNA', '-', 'Comissão', 'Nacional', 'de', 'Acreditação', 'Idioma/Language:', 'Home', 'CBR', 'RB', 'History', 'Editorial', 'Board', 'Publication', 'Guidelines', 'Advanced', 'Search', 'Current', 'Issue', 'Previous', 'Issues', 'Paper', 'Submission', 'Audience', 'Supplements', 'Ahead', 'of', 'Print', 'Contact', 'us', 'Vol.', '45', 'nº', '3', '-', 'Maio', '/', 'Jun.', 'of', '2012', 'EDITORIAL', 'Print', 'Page(s)', 'V', 'to', 'VI', 'Contrastes', 'orais', 'neutros', 'para', 'enterografia', 'por', 'tomografia', 'computadorizada', 'Autho(rs):', 'Daniella', 'Braz', 'Parente', 'PDF', 'Português', 'PDF', 'English', 'Texto', 'em', 'Português', 'English', 'Text', 'Durante', 'muitos', 'anos', 'o', 'intestino', 'delgado', 'foi', 'um', 'órgão', 'difícil', 'de', 'ser', 'estudado.', 'Até', 'bem', 'pouco', 'tempo,', 'os', 'únicos', 'métodos', 'de', 'imagem', 'disponíveis', 'para', 'o', 'estudo', 'deste', 'segmento', 'eram', 'o', 'trânsito', 'delgado', 'e', 'a', 'enteróclise', 'convencional,', 'que', 'apresentam', 'diversas', 'limitações(1,2).', 'Nos', 'últimos', 'anos,', 'com', 'o', 'desenvolvimento', 'da', 'tecnologia,', 'vários', 'métodos', 'têm', 'sido', 'desenvolvidos', 'para', 'o', 'estudo', 'do', 'intestino', 'delgado.', 'No', 'campo', 'da', 'endoscopia,', 'dispõe-se', 'hoje', 'da', 'cápsula', 'endoscópica(3)', 'e', 'da', 'enteroscopia', 'com', 'duplo-balão(4),', 'métodos', 'caros', 'e', 'não', 'amplamente', 'disponíveis', 'em', 'nosso', 'meio.', 'No', 'campo', 'da', 'radiologia,', 'surgiram', 'a', 'enterografia', 'e', 'a', 'enteróclise', 'por', 'tomografia', 'computadorizada', '(TC)', 'ou', 'por', 'ressonância', 'magnética', '(RM),', 'que', 'combinam', 'as', 'vantagens', 'dos', 'métodos', 'radiológicos', 'convencionais', 'com', 'as', 'dos', 'métodos', 'seccionais,', 'ou', 'seja,', 'a', 'distensão', 'do', 'lúmen', 'intestinal', 'associada', 'à', 'visualização', 'multiplanar', 'de', 'toda', 'a', 'cavidade', 'abdominal', 'e', 'pélvica,', 'sem', 'sobreposição', 'de', 'imagens.', 'Com', 'estes', 'métodos,', 'toda', 'a', 'espessura', 'da', 'parede', 'intestinal', 'é', 'visualizada,', 'incluindo', 'mucosa,', 'submucosa,', 'muscular', 'e', 'serosa,', 'assim', 'como', 'gordura', 'perientérica,', 'vasos', 'retos,', 'linfonodos', 'e', 'padrão', 'de', 'realce', 'pelo', 'contraste,', 'o', 'que', 'permite', 'identificar', 'espessamentos', 'parietais,', 'sinais', 'inflamatórios,', 'fístulas,', 'coleções,', 'além', 'de', 'focos', 'de', 'sangramento', 'e', 'tumores,', 'entre', 'outros(2,5\\x967).', 'A', 'distensão', 'luminal', 'é', 'um', 'passo', 'fundamental', 'na', 'interpretação', 'do', 'estudo', 'do', 'intestino', 'delgado,', 'já', 'que', 'alças', 'colabadas', 'podem', 'esconder', 'ou', 'simular', 'segmentos', 'doentes(2,5\\x967).', 'Neste', 'número', 'da', 'Radiologia', 'Brasileira,', \"D'Ippolito\", 'et', 'al.(8)', 'avaliam', 'o', 'desempenho', 'de', 'diferentes', 'meios', 'de', 'contraste', 'orais', 'neutros,', 'comparando', 'a', 'capacidade', 'de', 'distensão', 'intestinal,', 'a', 'definição', 'da', 'parede', 'intestinal,', 'a', 'aceitação', 'e', 'os', 'efeitos', 'colaterais.', 'Os', 'autores', 'descrevem', 'o', 'estudo', 'de', '30', 'pacientes', 'submetidos', 'a', 'enterografia', 'por', 'TC,', 'randomizados', 'entre', 'três', 'tipos', 'de', 'contraste', 'oral', 'neutro', 'disponíveis', 'em', 'nosso', 'meio:', 'água,', 'leite', 'integral', 'e', 'polietilenoglicol', '(PEG).', 'Neste', 'estudo,', 'a', 'concentração', 'de', 'PEG', 'empregada', 'foi', 'inferior', 'à', 'utilizada', 'em', 'estudos', 'anteriores,', 'com', 'o', 'objetivo', 'de', 'reduzir', 'seus', 'efeitos', 'indesejados,', 'mas', 'mantendo', 'sua', 'capacidade', 'de', 'distensão', 'da', 'luz', 'intestinal.', 'O', 'preparo', 'com', 'PEG', 'resultou', 'na', 'melhor', 'distensão', 'intestinal', '(57,5%),', 'não', 'havendo', 'diferença', 'significativa', 'entre', 'o', 'leite', '(35%)', 'e', 'a', 'água', '(25%).', 'Em', 'todos', 'os', 'grupos', 'foi', 'possível', 'a', 'definição', 'adequada', 'da', 'parede', 'intestinal', 'e', 'os', 'meios', 'de', 'contraste', 'tiveram', 'boa', 'aceitação.', 'Dos', 'pacientes', 'que', 'ingeriram', 'PEG,', '80%', 'apresentaram', 'diarreia,', 'efeito', 'colateral', 'não', 'observado', 'com', 'o', 'uso', 'de', 'leite', 'ou', 'água.', 'O', 'estudo', 'mostra,', 'de', 'forma', 'clara,', 'que', 'água', 'e', 'leite', 'não', 'são', 'bons', 'meios', 'de', 'contraste', 'oral', 'para', 'uso', 'na', 'enterografia,', 'já', 'que', 'são', 'absorvidos', 'pelo', 'intestino', 'e', 'não', 'distendem', 'adequadamente', 'as', 'alças,', 'o', 'que', 'é', 'muito', 'importante', 'para', 'a', 'correta', 'interpretação', 'do', 'exame.', 'Mostra', 'também', 'que', 'o', 'PEG', 'é', 'um', 'bom', 'meio', 'de', 'contraste', 'oral', 'para', 'o', 'estudo', 'do', 'intestino', 'delgado', 'e', 'que', 'os', 'pacientes', 'toleram', 'bem', 'a', 'ingestão', 'de', 'grande', 'volume', 'de', 'PEG', '(1.500', 'ml).', 'Além', 'disso,', 'demonstra', 'que', 'a', 'diarreia', 'é', 'um', 'efeito', 'indesejado', 'habitual,', 'que', 'ocorre', 'na', 'maioria', 'dos', 'pacientes,', 'inerente', 'à', 'propriedade', 'do', 'PEG', 'de', 'não', 'ser', 'absorvido', 'ao', 'longo', 'do', 'trato', 'gastrintestinal,', 'mas', 'geralmente', 'autolimitada.', 'Estudos', 'futuros', 'comparando', 'o', 'uso', 'de', 'PEG', 'em', 'baixa', 'concentração', 'com', 'PEG', 'na', 'concentração', 'habitual', 'são', 'necessários', 'para', 'melhor', 'avaliação', 'da', 'distensibilidade', 'luminal', 'e', 'incidência', 'de', 'efeitos', 'colaterais,', 'principalmente', 'diarreia.', 'Também', 'seria', 'interessante', 'a', 'realização', 'de', 'estudos', 'comparando', 'o', 'PEG', 'com', 'o', 'manitol,', 'já', 'que', 'este', 'último', 'tem', 'um', 'custo', 'muito', 'baixo', 'e', 'é', 'amplamente', 'disponível', 'na', 'rede', 'hospitalar', 'pública.', 'A', 'avaliação', 'de', 'pacientes', 'com', 'suspeita', 'ou', 'diagnóstico', 'de', 'doença', 'de', 'Crohn', 'é', 'uma', 'das', 'principais', 'indicações', 'de', 'estudo', 'do', 'intestino', 'delgado.', 'São', 'pacientes', 'jovens,', 'que', 'necessitam', 'de', 'reavaliações', 'durante', 'toda', 'a', 'vida.', 'Desta', 'forma,', 'deve', 'ser', 'levada', 'em', 'consideração', 'a', 'dose', 'de', 'radiação', 'a', 'que', 'estes', 'pacientes', 'são', 'submetidos', 'e', 'tentar', 'sempre', 'reduzir', 'ao', 'máximo', 'a', 'dose', 'de', 'radiação', 'utilizada(9).', 'Este', 'aspecto', 'foi', 'contemplado', 'no', 'estudo', 'de', \"D'Ippolito\", 'et', 'al.(8),', 'no', 'qual', 'foi', 'realizada', 'apenas', 'a', 'fase', 'enterográfica,', 'já', 'que', 'as', 'fases', 'pré-contraste,', 'arterial', 'e', 'tardia', 'não', 'acrescentam', 'significativamente', 'para', 'o', 'diagnóstico', 'e', 'quadriplicam', 'a', 'dose', 'de', 'radiação', 'a', 'que', 'os', 'pacientes', 'são', 'expostos.', 'Outra', 'opção', 'para', 'esta', 'população', 'é', 'a', 'enterografia', 'por', 'RM,', 'que', 'tem', 'acurácia', 'diagnóstica', 'semelhante', 'e', 'a', 'vantagem', 'de', 'não', 'utilizar', 'radiação,', 'podendo', 'ser', 'repetida', 'múltiplas', 'vezes', 'sem', 'malefícios', 'ao', 'paciente(6).', 'A', 'comparação', 'entre', 'diferentes', 'tipos', 'de', 'contraste', 'oral', 'disponíveis', 'em', 'nosso', 'meio', 'para', 'uso', 'na', 'enterografia', 'por', 'TC', 'é', 'um', 'tema', 'bastante', 'importante,', 'visto', 'a', 'grande', 'prevalência', 'de', 'pacientes', 'com', 'doenças', 'no', 'intestino', 'delgado', 'que', 'podem', 'se', 'beneficiar', 'deste', 'método', 'de', 'imagem,', 'como', 'pacientes', 'com', 'doença', 'inflamatória', 'intestinal,', 'notadamente', 'os', 'com', 'doença', 'de', 'Crohn,', 'pacientes', 'com', 'síndrome', 'do', 'cólon', 'irritável', 'que', 'precisam', 'excluir', 'outras', 'causas', 'para', 'a', 'dor', 'abdominal,', 'os', 'com', 'sangramento', 'intestinal', 'e', 'os', 'com', 'tumores', 'do', 'intestino', 'delgado.', 'A', 'TC', 'está', 'amplamente', 'disponível', 'em', 'nosso', 'país', 'e', 'é', 'importante', 'que', 'os', 'radiologistas', 'estejam', 'familiarizados', 'com', 'este', 'método,', 'para', 'contribuir', 'de', 'forma', 'significativa', 'no', 'diagnóstico,', 'tratamento', 'e', 'acompanhamento', 'dos', 'pacientes', 'com', 'doenças', 'intestinais.', 'REFERÊNCIAS', '1.', 'Saibeni', 'S,', 'Rondonotti', 'E,', 'Iozzelli', 'A,', 'et', 'al.', 'Imaging', 'of', 'the', 'small', 'bowel', 'in', \"Crohn's\", 'disease:', 'a', 'review', 'of', 'old', 'and', 'new', 'techniques.', 'World', 'J', 'Gastroenterol.', '2007;13:3279\\x9687.', '2.', 'Lee', 'SS,', 'Kim', 'AY,', 'Yang', 'SK,', 'et', 'al.', 'Crohn', 'disease', 'of', 'the', 'small', 'bowel:', 'comparison', 'of', 'CT', 'enterography,', 'MR', 'enterography,', 'and', 'small-bowel', 'follow-through', 'as', 'diagnostic', 'techniques.', 'Radiology.', '2009;251:751\\x9661.', '3.', 'Freitas', 'GP,', 'Teixeira', 'N,', 'Feldman', 'G.', 'Capsule', 'endoscopy', 'in', 'clinical', 'practice:', 'four', 'years', 'of', 'experience', 'from', 'a', 'single', 'center.', 'Arq', 'Gastroenterol.', '2011;48:220\\x962.', '4.', 'Albert', 'JG.', 'Interventional', 'balloon-enteroscopy.', 'J', 'Interv', 'Gastroenterol.', '2012;2:42\\x9650.', '5.', 'Costa-Silva', 'L,', 'Martins', 'T,', 'Passos', 'MCF.', 'Enterografia', 'por', 'tomografia', 'computadorizada:', 'experiência', 'inicial', 'na', 'avaliação', 'das', 'doenças', 'do', 'intestino', 'delgado.', 'Radiol', 'Bras.', '2010;43:303\\x968.', '6.', 'Siddiki', 'HA,', 'Fidler', 'JL,', 'Fletcher', 'JG,', 'et', 'al.', 'Prospective', 'comparison', 'of', 'state-of-the-art', 'MR', 'enterography', 'and', 'CT', 'enterography', 'in', 'small-bowel', \"Crohn's\", 'disease.', 'AJR', 'Am', 'J', 'Roentgenol.', '2009;193:113\\x9621.', '7.', 'Masselli', 'G,', 'Gualdi', 'G.', 'Evaluation', 'of', 'small', 'bowel', 'tumors:', 'MR', 'entero\\xadclysis.', 'Abdom', 'Imaging.', '2010;35:23\\x9630.', '8.', \"D'Ippolito\", 'G,', 'Braga', 'FA,', 'Resende', 'MC,', 'et', 'al.', 'Enterografia', 'por', 'tomografia', 'computadorizada:', 'uma', 'avaliação', 'de', 'diferentes', 'contrastes', 'orais', 'neutros.', 'Radiol', 'Bras.', '2012;45:139\\x9643.', '9.', 'Desmond', 'AN,', \"O'Regan\", 'K,', 'Curran', 'C,', 'et', 'al.', \"Crohn's\", 'disease:', 'factors', 'associated', 'with', 'exposure', 'to', 'high', 'levels', 'of', 'diagnostic', 'radiation.', 'Gut.', '2008;57:1524\\x969.', 'Doutoranda', 'em', 'Radiologia', 'da', 'Universidade', 'Federal', 'do', 'Rio', 'de', 'Janeiro', '(UFRJ),', 'Pesquisadora', 'do', 'Instituto', \"D'Or\", 'de', 'Pesquisa', 'e', 'Ensino,', 'Radiologista', 'da', 'Rede', 'Labs', \"D'Or,\", 'Rio', 'de', 'Janeiro,', 'RJ,', 'Brasil.', 'E-mail:', 'daniella.parente@gmail.com', 'Back', 'Top', 'Print', 'GN1©', 'Copyright', '2020', '-', 'All', 'rights', 'reserved', 'to', 'Colégio', 'Brasileiro', 'de', 'Radiologia', 'e', 'Diagnóstico', 'por', 'ImagemAv.', 'Paulista,', '37', '-', '7°', 'andar', '-', 'Conj.', '71', '-', 'CEP', '01311-902', '-', 'São', 'Paulo', '-', 'SP', '-', 'Brazil', '-', 'Phone:', '(11)', '3372-4544', '-', 'Fax:', '(11)', '3372-4554']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-ed6866f107e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-2575344b2c2f>\u001b[0m in \u001b[0;36mextract_words\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Remove any punctuation characters present in the beginning of the word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: string index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx3sa_mgng32",
        "colab_type": "text"
      },
      "source": [
        "As we saw during the video, this is a wasteful way to represent text. We can be much more efficient by representing each word by a number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOEjE2qing33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_dict = {}\n",
        "word_list = []\n",
        "vocabulary_size = 0\n",
        "text_tokens = []\n",
        "\n",
        "for word in text_words:\n",
        "    # If we are seeing this word for the first time, create an id for it and added it to our word dictionary\n",
        "    if word not in word_dict:\n",
        "        word_dict[word] = vocabulary_size\n",
        "        word_list.append(word)\n",
        "        vocabulary_size += 1\n",
        "    \n",
        "    # add the token corresponding to the current word to the tokenized text.\n",
        "    text_tokens.append(word_dict[word])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZK1AIO1ng35",
        "colab_type": "text"
      },
      "source": [
        "When we were tokenizing our text, we also generated a dictionary **word_dict** that maps words to integers and a **word_list** that maps each integer to the corresponding word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HopXhHkang35",
        "colab_type": "code",
        "outputId": "fdc2dc13-a804-445b-ad77-5f132c7764c3",
        "colab": {}
      },
      "source": [
        "print(\"Word list:\", word_list, \"\\n\\n Word dictionary:\")\n",
        "pprint(word_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word list: ['mary', 'had', 'a', 'little', 'lamb', 'whose', 'fleece', 'was', 'white', 'as', 'snow', 'and', 'everywhere', 'that', 'went', 'the', 'sure', 'to', 'go'] \n",
            "\n",
            " Word dictionary:\n",
            "{'a': 2,\n",
            " 'and': 11,\n",
            " 'as': 9,\n",
            " 'everywhere': 12,\n",
            " 'fleece': 6,\n",
            " 'go': 18,\n",
            " 'had': 1,\n",
            " 'lamb': 4,\n",
            " 'little': 3,\n",
            " 'mary': 0,\n",
            " 'snow': 10,\n",
            " 'sure': 16,\n",
            " 'that': 13,\n",
            " 'the': 15,\n",
            " 'to': 17,\n",
            " 'was': 7,\n",
            " 'went': 14,\n",
            " 'white': 8,\n",
            " 'whose': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB688ddCng37",
        "colab_type": "text"
      },
      "source": [
        "These two datastructures already proved their usefulness when we converted our text to a list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "layQT9zhng38",
        "colab_type": "code",
        "outputId": "25dffbbe-368c-485b-b589-4008a62b8534",
        "colab": {}
      },
      "source": [
        "print(text_tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 3, 4, 3, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0, 14, 0, 14, 0, 14, 12, 13, 0, 14, 15, 4, 7, 16, 17, 18]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg7BRDIlng39",
        "colab_type": "text"
      },
      "source": [
        "Unfortunately, while this representation is convenient for memory reasons it has some severe limitations. Perhaps the most important of which is the fact that computers naturally assume that numbers can be operated on mathematically (by addition, subtraction, etc) in a way that doesn't match our understanding of words.\n",
        "\n",
        "## One-hot encoding\n",
        "\n",
        "One typical way of overcoming this difficulty is to represent each word by a one-hot encoded vector where every element is zero except the one corresponding to a specific word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOvE43iNng3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(word, word_dict):\n",
        "    \"\"\"\n",
        "        Generate a one-hot encoded vector corresponding to *word*\n",
        "    \"\"\"\n",
        "    \n",
        "    vector = np.zeros(len(word_dict))\n",
        "    vector[word_dict[word]] = 1\n",
        "    \n",
        "    return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJbcRHF_ng3_",
        "colab_type": "text"
      },
      "source": [
        "So, for example, the word \"fleece\" would be represented by:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aosVC-Nrng3_",
        "colab_type": "code",
        "outputId": "5a1e89a0-25f6-4ae1-c991-a99b73381770",
        "colab": {}
      },
      "source": [
        "fleece_hot = one_hot(\"fleece\", word_dict)\n",
        "print(fleece_hot)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G34p_53xng4B",
        "colab_type": "text"
      },
      "source": [
        "This vector has every element set to zero, except element 6, since:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYrNuE_sng4C",
        "colab_type": "code",
        "outputId": "2a3641c2-c12a-44d6-fbef-a72c20154fd2",
        "colab": {}
      },
      "source": [
        "print(word_dict[\"fleece\"])\n",
        "fleece_hot[6] == 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0iUMdcqng4D",
        "colab_type": "text"
      },
      "source": [
        "## Bag of words\n",
        "\n",
        "We can now use the one-hot encoded vector for each word to produce a vector representation of our original text, by simply adding up all the one-hot encoded vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUYRsnXQng4E",
        "colab_type": "code",
        "outputId": "8ee7cd48-a53f-4aba-d934-a76efa43957e",
        "colab": {}
      },
      "source": [
        "text_vector1 = np.zeros(vocabulary_size)\n",
        "\n",
        "for word in text_words:\n",
        "    hot_word = one_hot(word, word_dict)\n",
        "    text_vector1 += hot_word\n",
        "    \n",
        "print(text_vector1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6. 2. 2. 4. 5. 1. 1. 2. 1. 1. 1. 1. 2. 2. 4. 1. 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFso61uAng4F",
        "colab_type": "text"
      },
      "source": [
        "In practice, we can also easily skip the encoding step at the word level by using the *word_dict* defined above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AchFtUqng4G",
        "colab_type": "code",
        "outputId": "a4b0586e-b103-48ea-8ee4-d216395f116c",
        "colab": {}
      },
      "source": [
        "text_vector = np.zeros(vocabulary_size)\n",
        "\n",
        "for word in text_words:\n",
        "    text_vector[word_dict[word]] += 1\n",
        "    \n",
        "print(text_vector)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6. 2. 2. 4. 5. 1. 1. 2. 1. 1. 1. 1. 2. 2. 4. 1. 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKOlSrasng4I",
        "colab_type": "text"
      },
      "source": [
        "Naturally, this approach is completely equivalent to the previous one and has the added advantage of being more efficient in terms of both speed and memory requirements.\n",
        "\n",
        "This is known as the __bag of words__ representation of the text. It should be noted that these vectors simply contains the number of times each word appears in our document, so we can easily tell that the word *mary* appears exactly 6 times in our little nursery rhyme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hltKo6K0ng4I",
        "colab_type": "code",
        "outputId": "05ae4ce2-de30-48e0-e3de-99c50a971a9b",
        "colab": {}
      },
      "source": [
        "text_vector[word_dict[\"mary\"]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0rIQU1cng4K",
        "colab_type": "text"
      },
      "source": [
        "A more pythonic (and efficient) way of producing the same result is to use the standard __Counter__ module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nujnpDV6ng4K",
        "colab_type": "code",
        "outputId": "ae332765-4ffb-4a4e-9d5c-05a5a80fa3b6",
        "colab": {}
      },
      "source": [
        "word_counts = Counter(text_words)\n",
        "pprint(word_counts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'mary': 6,\n",
            "         'lamb': 5,\n",
            "         'little': 4,\n",
            "         'went': 4,\n",
            "         'had': 2,\n",
            "         'a': 2,\n",
            "         'was': 2,\n",
            "         'everywhere': 2,\n",
            "         'that': 2,\n",
            "         'whose': 1,\n",
            "         'fleece': 1,\n",
            "         'white': 1,\n",
            "         'as': 1,\n",
            "         'snow': 1,\n",
            "         'and': 1,\n",
            "         'the': 1,\n",
            "         'sure': 1,\n",
            "         'to': 1,\n",
            "         'go': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO5Chjoong4M",
        "colab_type": "text"
      },
      "source": [
        "From which we can easily generate the __text_vector__ and __word_dict__ data structures:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWAO_mlVng4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "items = list(word_counts.items())\n",
        "\n",
        "# Extract word dictionary and vector representation\n",
        "word_dict2 = dict([[items[i][0], i] for i in range(len(items))])\n",
        "text_vector2 = [items[i][1] for i in range(len(items))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01aTDZaDng4O",
        "colab_type": "text"
      },
      "source": [
        "And let's take a look at them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0EbtMBRng4O",
        "colab_type": "code",
        "outputId": "48e9a218-1e75-4cc0-a102-e7ce93ebd99b",
        "colab": {}
      },
      "source": [
        "print(\"Text vector:\", text_vector2, \"\\n\\nWord dictionary:\")\n",
        "pprint(word_dict2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text vector: [6, 2, 2, 4, 5, 1, 1, 2, 1, 1, 1, 1, 2, 2, 4, 1, 1, 1, 1] \n",
            "\n",
            "Word dictionary:\n",
            "{'a': 2,\n",
            " 'and': 11,\n",
            " 'as': 9,\n",
            " 'everywhere': 12,\n",
            " 'fleece': 6,\n",
            " 'go': 18,\n",
            " 'had': 1,\n",
            " 'lamb': 4,\n",
            " 'little': 3,\n",
            " 'mary': 0,\n",
            " 'snow': 10,\n",
            " 'sure': 16,\n",
            " 'that': 13,\n",
            " 'the': 15,\n",
            " 'to': 17,\n",
            " 'was': 7,\n",
            " 'went': 14,\n",
            " 'white': 8,\n",
            " 'whose': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1prvuiwng4P",
        "colab_type": "text"
      },
      "source": [
        "The results using this approach are slightly different than the previous ones, because the words are mapped to different integer ids but the corresponding values are the same:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUnz1CpAng4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word in word_dict.keys():\n",
        "    if text_vector[word_dict[word]] != text_vector2[word_dict2[word]]:\n",
        "        print(\"Error!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPXzbIMing4R",
        "colab_type": "text"
      },
      "source": [
        "As expected, there are no differences!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpQXkbflng4R",
        "colab_type": "text"
      },
      "source": [
        "## Term Frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAUBQZQqng4R",
        "colab_type": "text"
      },
      "source": [
        "The bag of words vector representation introduced above relies simply on the frequency of occurence of each word. Following a long tradition of giving fancy names to simple ideas, this is known as __Term Frequency__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjztfZfbng4S",
        "colab_type": "text"
      },
      "source": [
        "Intuitively, we expect the the frequency with which a given word is mentioned should correspond to the relevance of that word for the piece of text we are considering. For example, **Mary** is a pretty important word in our little nursery rhyme and indeed it is the one that occurs the most often:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3KgToNDng4S",
        "colab_type": "code",
        "outputId": "09d3addd-839a-437d-f0d0-5c7d99210552",
        "colab": {}
      },
      "source": [
        "sorted(items, key=lambda x:x[1], reverse=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('mary', 6),\n",
              " ('lamb', 5),\n",
              " ('little', 4),\n",
              " ('went', 4),\n",
              " ('had', 2),\n",
              " ('a', 2),\n",
              " ('was', 2),\n",
              " ('everywhere', 2),\n",
              " ('that', 2),\n",
              " ('whose', 1),\n",
              " ('fleece', 1),\n",
              " ('white', 1),\n",
              " ('as', 1),\n",
              " ('snow', 1),\n",
              " ('and', 1),\n",
              " ('the', 1),\n",
              " ('sure', 1),\n",
              " ('to', 1),\n",
              " ('go', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2--DCh6ng4T",
        "colab_type": "text"
      },
      "source": [
        "However, it's hard to draw conclusions from such a small piece of text. Let us consider a significantly larger piece of text, the first 100 MB of the english Wikipedia from: http://mattmahoney.net/dc/textdata. For the sake of convenience, text8.gz has been included in this repository in the **data/** directory. We start by loading it's contents into memory as an array of words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODgqkggFng4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = []\n",
        "\n",
        "for line in gzip.open(\"data/text8.gz\", 'rt'):\n",
        "    data.extend(line.strip().split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzs-2d8Qng4V",
        "colab_type": "text"
      },
      "source": [
        "Now let's take a look at the most common words in this large corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqKh8k4ong4V",
        "colab_type": "code",
        "outputId": "bc52efea-0d1d-4102-e7a3-aad023861146",
        "colab": {}
      },
      "source": [
        "counts = Counter(data)\n",
        "\n",
        "sorted_counts = sorted(list(counts.items()), key=lambda x:x[1], reverse=True)\n",
        "\n",
        "for word, count in sorted_counts[:10]:\n",
        "    print(word, count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the 1061396\n",
            "of 593677\n",
            "and 416629\n",
            "one 411764\n",
            "in 372201\n",
            "a 325873\n",
            "to 316376\n",
            "zero 264975\n",
            "nine 250430\n",
            "two 192644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzWBNbUVng4W",
        "colab_type": "text"
      },
      "source": [
        "Surprisingly, we find that the most common words are not particularly meaningful. Indeed, this is a common occurence in Natural Language Processing. The most frequent words are typically auxiliaries required due to gramatical rules.\n",
        "\n",
        "On the other hand, there is also a large number of words that occur very infrequently as can be easily seen by glancing at the word freqency distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IAtLIvung4X",
        "colab_type": "code",
        "outputId": "cbbd5464-cca5-4249-fcee-4bfd9b71490c",
        "colab": {}
      },
      "source": [
        "dist = Counter(counts.values())\n",
        "dist = list(dist.items())\n",
        "dist.sort(key=lambda x:x[0])\n",
        "dist = np.array(dist)\n",
        "\n",
        "norm = np.dot(dist.T[0], dist.T[1])\n",
        "\n",
        "plt.loglog(dist.T[0], dist.T[1]/norm)\n",
        "plt.xlabel(\"count\")\n",
        "plt.ylabel(\"P(count)\")\n",
        "plt.title(\"Word frequency distribution\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Word frequency distribution')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9z8q7i3ng4Y",
        "colab_type": "text"
      },
      "source": [
        "## Stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njulRppyng4Z",
        "colab_type": "text"
      },
      "source": [
        "One common technique to simplify NLP tasks is to remove what are known as Stopwords, words that are very frequent but not meaningful. If we simply remove the most common 100 words, we significantly reduce the amount of data we have to consider while losing little information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jQqP2cJng4Z",
        "colab_type": "code",
        "outputId": "253d2887-6351-40b5-e1b7-15e0b4d761e0",
        "colab": {}
      },
      "source": [
        "stopwords = set([word for word, count in sorted_counts[:100]])\n",
        "\n",
        "clean_data = []\n",
        "\n",
        "for word in data:\n",
        "    if word not in stopwords:\n",
        "        clean_data.append(word)\n",
        "\n",
        "print(\"Original size:\", len(data))\n",
        "print(\"Clean size:\", len(clean_data))\n",
        "print(\"Reduction:\", 1-len(clean_data)/len(data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original size: 17005207\n",
            "Clean size: 9006229\n",
            "Reduction: 0.470384041782026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFh57UmPng4b",
        "colab_type": "text"
      },
      "source": [
        "Wow, our dataset size was reduced almost in half!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6VbN_mgng4b",
        "colab_type": "text"
      },
      "source": [
        "In practice, we don't simply remove the most common words in our corpus but rather a manually curate list of stopwords. Lists for dozens of languages and applications can easily be found online."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cf2lGs4ng4b",
        "colab_type": "text"
      },
      "source": [
        "## Term Frequency/Inverse Document Frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK3EFPxmng4c",
        "colab_type": "text"
      },
      "source": [
        "One way of determining of the relative importance of a word is to see how often it appears across multiple documents. Words that are relevant to a specific topic are more likely to appear in documents about that topic and much less in documents about other topics. On the other hand, less meaningful words (like **the**) will be common across documents about any subject."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0amk9Hsing4c",
        "colab_type": "text"
      },
      "source": [
        "To measure the document frequency of a word we will need to have multiple documents. For the sake of simplicity, we will treat each sentence of our nursery rhyme as an individual document:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3jVANDtng4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_text = text.split('.')\n",
        "corpus_words = []\n",
        "\n",
        "for document in corpus_text:\n",
        "    doc_words = extract_words(document)\n",
        "    corpus_words.append(doc_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyLP0uZOng4d",
        "colab_type": "text"
      },
      "source": [
        "Now our corpus is represented as a list of word lists, where each list is just the word representation of the corresponding sentence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vy_haEvng4e",
        "colab_type": "code",
        "outputId": "19016832-c021-44c8-ef8e-0b6f2c06a6da",
        "colab": {}
      },
      "source": [
        "pprint(corpus_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['mary', 'had', 'a', 'little', 'lamb', 'little', 'lamb', 'little', 'lamb'],\n",
            " ['mary',\n",
            "  'had',\n",
            "  'a',\n",
            "  'little',\n",
            "  'lamb',\n",
            "  'whose',\n",
            "  'fleece',\n",
            "  'was',\n",
            "  'white',\n",
            "  'as',\n",
            "  'snow'],\n",
            " ['and', 'everywhere', 'that', 'mary', 'went', 'mary', 'went', 'mary', 'went'],\n",
            " ['everywhere',\n",
            "  'that',\n",
            "  'mary',\n",
            "  'went',\n",
            "  'the',\n",
            "  'lamb',\n",
            "  'was',\n",
            "  'sure',\n",
            "  'to',\n",
            "  'go']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGXJZL1nng4f",
        "colab_type": "text"
      },
      "source": [
        "Let us now calculate the number of documents in which each word appears:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFqB0-k0ng4f",
        "colab_type": "code",
        "outputId": "a64e82be-8845-4c47-ad34-7789d5be18a7",
        "colab": {}
      },
      "source": [
        "document_count = {}\n",
        "\n",
        "for document in corpus_words:\n",
        "    word_set = set(document)\n",
        "    \n",
        "    for word in word_set:\n",
        "        document_count[word] = document_count.get(word, 0) + 1\n",
        "\n",
        "pprint(document_count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 2,\n",
            " 'and': 1,\n",
            " 'as': 1,\n",
            " 'everywhere': 2,\n",
            " 'fleece': 1,\n",
            " 'go': 1,\n",
            " 'had': 2,\n",
            " 'lamb': 3,\n",
            " 'little': 2,\n",
            " 'mary': 4,\n",
            " 'snow': 1,\n",
            " 'sure': 1,\n",
            " 'that': 2,\n",
            " 'the': 1,\n",
            " 'to': 1,\n",
            " 'was': 2,\n",
            " 'went': 2,\n",
            " 'white': 1,\n",
            " 'whose': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RelHDaeIng4h",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the word __Mary__ appears in all 4 of our documents, making it useless when it comes to distinguish between the different sentences. On the other hand, words like __white__ which appear in only one document are very discriminative. Using this approach we can define a new quantity, the ___Inverse Document Frequency__ that tells us how frequent a word is across the documents in a specific corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAkhvtiMng4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inv_doc_freq(corpus_words):\n",
        "    number_docs = len(corpus_words)\n",
        "    \n",
        "    document_count = {}\n",
        "\n",
        "    for document in corpus_words:\n",
        "        word_set = set(document)\n",
        "\n",
        "        for word in word_set:\n",
        "            document_count[word] = document_count.get(word, 0) + 1\n",
        "    \n",
        "    IDF = {}\n",
        "    \n",
        "    for word in document_count:\n",
        "        IDF[word] = np.log(number_docs/document_count[word])\n",
        "        \n",
        "    \n",
        "    return IDF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpmCqaUCng4i",
        "colab_type": "text"
      },
      "source": [
        "Where we followed the convention of using the logarithm of the inverse document frequency. This has the numerical advantage of avoiding to have to handle small fractional numbers. \n",
        "\n",
        "We can easily see that the IDF gives a smaller weight to the most common words and a higher weight to the less frequent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bjGj4mEng4i",
        "colab_type": "code",
        "outputId": "518124a8-a3f9-4cef-b29a-0460c9d78711",
        "colab": {}
      },
      "source": [
        "IDF = inv_doc_freq(corpus_words)\n",
        "\n",
        "pprint(IDF)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 0.6931471805599453,\n",
            " 'and': 1.3862943611198906,\n",
            " 'as': 1.3862943611198906,\n",
            " 'everywhere': 0.6931471805599453,\n",
            " 'fleece': 1.3862943611198906,\n",
            " 'go': 1.3862943611198906,\n",
            " 'had': 0.6931471805599453,\n",
            " 'lamb': 0.28768207245178085,\n",
            " 'little': 0.6931471805599453,\n",
            " 'mary': 0.0,\n",
            " 'snow': 1.3862943611198906,\n",
            " 'sure': 1.3862943611198906,\n",
            " 'that': 0.6931471805599453,\n",
            " 'the': 1.3862943611198906,\n",
            " 'to': 1.3862943611198906,\n",
            " 'was': 0.6931471805599453,\n",
            " 'went': 0.6931471805599453,\n",
            " 'white': 1.3862943611198906,\n",
            " 'whose': 1.3862943611198906}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTHOqs3eng4k",
        "colab_type": "text"
      },
      "source": [
        "As expected **Mary** has the smallest weight of all words 0, meaning that it is effectively removed from the dataset. You can consider this as a way of implicitly identify and remove stopwords. In case you do want to keep even the words that appear in every document, you can just add a 1. to the argument of the logarithm above:\n",
        "\n",
        "\\begin{equation}\n",
        "\\log\\left[1+\\frac{N_d}{N_d\\left(w\\right)}\\right]\n",
        "\\end{equation}\n",
        "\n",
        "When we multiply the term frequency of each word by it's inverse document frequency, we have a good way of quantifying how relevant a word is to understand the meaning of a specific document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPGE-48Cng4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_idf(corpus_words):\n",
        "    IDF = inv_doc_freq(corpus_words)\n",
        "    \n",
        "    TFIDF = []\n",
        "    \n",
        "    for document in corpus_words:\n",
        "        TFIDF.append(Counter(document))\n",
        "    \n",
        "    for document in TFIDF:\n",
        "        for word in document:\n",
        "            document[word] = document[word]*IDF[word]\n",
        "            \n",
        "    return TFIDF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsR-_-kNng4l",
        "colab_type": "code",
        "outputId": "2b2656fe-08e6-475f-88dc-873a9964b7ff",
        "colab": {}
      },
      "source": [
        "tf_idf(corpus_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Counter({'mary': 0.0,\n",
              "          'had': 0.6931471805599453,\n",
              "          'a': 0.6931471805599453,\n",
              "          'little': 2.0794415416798357,\n",
              "          'lamb': 0.8630462173553426}),\n",
              " Counter({'mary': 0.0,\n",
              "          'had': 0.6931471805599453,\n",
              "          'a': 0.6931471805599453,\n",
              "          'little': 0.6931471805599453,\n",
              "          'lamb': 0.28768207245178085,\n",
              "          'whose': 1.3862943611198906,\n",
              "          'fleece': 1.3862943611198906,\n",
              "          'was': 0.6931471805599453,\n",
              "          'white': 1.3862943611198906,\n",
              "          'as': 1.3862943611198906,\n",
              "          'snow': 1.3862943611198906}),\n",
              " Counter({'and': 1.3862943611198906,\n",
              "          'everywhere': 0.6931471805599453,\n",
              "          'that': 0.6931471805599453,\n",
              "          'mary': 0.0,\n",
              "          'went': 2.0794415416798357}),\n",
              " Counter({'everywhere': 0.6931471805599453,\n",
              "          'that': 0.6931471805599453,\n",
              "          'mary': 0.0,\n",
              "          'went': 0.6931471805599453,\n",
              "          'the': 1.3862943611198906,\n",
              "          'lamb': 0.28768207245178085,\n",
              "          'was': 0.6931471805599453,\n",
              "          'sure': 1.3862943611198906,\n",
              "          'to': 1.3862943611198906,\n",
              "          'go': 1.3862943611198906})]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_OSm-Fmng4n",
        "colab_type": "text"
      },
      "source": [
        "Now we finally have a vector representation of each of our documents that takes the informational contributions of each word into account. Each of these vectors provides us with a unique representation of each document, in the context (corpus) in which it occurs, making it posssible to define the similarity of two documents, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xblKMB-ng4n",
        "colab_type": "text"
      },
      "source": [
        "## Porter Stemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6059OkXng4n",
        "colab_type": "text"
      },
      "source": [
        "There is still, however, one issue with our approach to representing text. Since we treat each word as a unique token and completely independently from all others, for large documents we will end up with many variations of the same word such as verb conjugations, the corresponding adverbs and nouns, etc. \n",
        "\n",
        "One way around this difficulty is to use stemming algorithm to reduce words to their root (or stem) version. The most famous Stemming algorithm is known as the **Porter Stemmer** and was introduced by Martin Porter in 1980 [Program 14, 130 (1980)](https://dl.acm.org/citation.cfm?id=275705)\n",
        "\n",
        "The algorithm starts by defining consonants (C) and vowels (V):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnfdiZwXng4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "V = set('aeiouy')\n",
        "C = set('bcdfghjklmnpqrstvwxz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehZ4W6mxng4p",
        "colab_type": "text"
      },
      "source": [
        "The stem of a word is what is left of that word after a speficic ending has been removed. A function to do this is easy to implement:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXDSWtmMng4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_stem(suffix, word):\n",
        "    \"\"\"\n",
        "        Extract the stem of a word\n",
        "    \"\"\"\n",
        "    \n",
        "    if word.lower().endswith(suffix.lower()): # Case insensitive comparison\n",
        "        return word[:-len(suffix)]\n",
        "\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z5-MbNrng4q",
        "colab_type": "text"
      },
      "source": [
        "It also defines words (or stems) to be sequences of vowels and consonants of the form:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-EoYKsYng4r",
        "colab_type": "text"
      },
      "source": [
        "\\begin{equation}\n",
        "[C](VC)^m[V]\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHPCxyyHng4r",
        "colab_type": "text"
      },
      "source": [
        "where $m$ is called the **measure** of the word and [] represent optional sections. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWPtbgbfng4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def measure(orig_word):\n",
        "    \"\"\"\n",
        "        Calculate the \"measure\" m of a word or stem, according to the Porter Stemmer algorthim\n",
        "    \"\"\"\n",
        "    \n",
        "    word = orig_word.lower()\n",
        "\n",
        "    optV = False\n",
        "    optC = False\n",
        "    VC = False\n",
        "    m = 0\n",
        "\n",
        "    pos = 0\n",
        "\n",
        "    # We can think of this implementation as a simple finite state machine\n",
        "    # looks for sequences of vowels or consonants depending of the state\n",
        "    # in which it's in, while keeping track of how many VC sequences it\n",
        "    # has encountered.\n",
        "    # The presence of the optional V and C portions is recorded in the\n",
        "    # optV and optC booleans.\n",
        "    \n",
        "    # We're at the initial state.\n",
        "    # gobble up all the optional consonants at the beginning of the word\n",
        "    while pos < len(word) and word[pos] in C:\n",
        "        pos += 1\n",
        "        optC = True\n",
        "\n",
        "    while pos < len(word):\n",
        "        # Now we know that the next state must be a vowel\n",
        "        while pos < len(word) and word[pos] in V:\n",
        "            pos += 1\n",
        "            optV = True\n",
        "\n",
        "        # Followd by a consonant\n",
        "        while pos < len(word) and word[pos] in C:\n",
        "            pos += 1\n",
        "            optV = False\n",
        "        \n",
        "        # If a consonant was found, then we matched VC\n",
        "        # so we should increment m by one. Otherwise, \n",
        "        # optV remained true and we simply had a dangling\n",
        "        # V sequence.\n",
        "        if not optV:\n",
        "            m += 1\n",
        "\n",
        "    return m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VzDrn6Cng4u",
        "colab_type": "text"
      },
      "source": [
        "Let's consider a simple example. The word __crepusculars__ should have measure 4:\n",
        "\n",
        "[cr] (ep) (usc) (ul) (ars)\n",
        "\n",
        "and indeed it does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpkAi1ekng4u",
        "colab_type": "code",
        "outputId": "a43c25a7-1d90-4b8f-f4d4-662a3d1ff1f1",
        "colab": {}
      },
      "source": [
        "word = \"crepusculars\"\n",
        "print(measure(word))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-CtZdUsng4w",
        "colab_type": "text"
      },
      "source": [
        "The Porter algorithm sequentially applies a series of transformation rules over a series of 5 steps (step 1 is divided in 3 substeps and step 5 in 2). The rules are only applied if a certain condition is true. \n",
        "\n",
        "In addition to possibily specifying a requirement on the measure of a word, conditions can make use of different boolean functions as well: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39sz-CKvng4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ends_with(char, stem):\n",
        "    \"\"\"\n",
        "        Checks the ending of the word\n",
        "    \"\"\"\n",
        "    return stem[-1] == char\n",
        "\n",
        "def double_consonant(stem):\n",
        "    \"\"\"\n",
        "        Checks the ending of a word for a double consonant\n",
        "    \"\"\"\n",
        "    if len(stem) < 2:\n",
        "        return False\n",
        "\n",
        "    if stem[-1] in C and stem[-2] == stem[-1]:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def contains_vowel(stem):\n",
        "    \"\"\"\n",
        "        Checks if a word contains a vowel or not\n",
        "    \"\"\"\n",
        "    return len(set(stem) & V) > 0 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnWHuI3Tng4z",
        "colab_type": "text"
      },
      "source": [
        "Finally, we define a function to apply a specific rule to a word or stem:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJP-wP4zng4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_rule(condition, suffix, replacement, word):\n",
        "    \"\"\"\n",
        "        Apply Porter Stemmer rule.\n",
        "        if \"condition\" is True replace \"suffix\" by \"replacement\" in \"word\"\n",
        "    \"\"\"\n",
        "    \n",
        "    stem = get_stem(suffix, word)\n",
        "\n",
        "    if stem is not None and condition is True:\n",
        "        # Remove the suffix\n",
        "        word = stem\n",
        "\n",
        "        # Add the replacement suffix, if any\n",
        "        if replacement is not None:\n",
        "            word += replacement\n",
        "\n",
        "    return word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYdDO_arng40",
        "colab_type": "text"
      },
      "source": [
        "Now we can see how rules can be applied. For example, this rule, from step 1b is successfully applied to __pastered__:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKrDDuRDng40",
        "colab_type": "code",
        "outputId": "fc33dd26-6b9e-4fd7-e36b-5e993addd46c",
        "colab": {}
      },
      "source": [
        "word = \"plastered\"\n",
        "suffix = \"ed\"\n",
        "stem = get_stem(suffix, word)\n",
        "apply_rule(contains_vowel(stem), suffix, None, word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'plaster'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rQC6wxNng42",
        "colab_type": "text"
      },
      "source": [
        "While try applying the same rule to **bled** will fail to pass the condition resulting in no change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e21gfyZqng42",
        "colab_type": "code",
        "outputId": "e2a13841-a570-49c2-dca7-7f5642f0d1cc",
        "colab": {}
      },
      "source": [
        "word = \"bled\"\n",
        "suffix = \"ed\"\n",
        "stem = get_stem(suffix, word)\n",
        "apply_rule(contains_vowel(stem), suffix, None, word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bled'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMuINvasng43",
        "colab_type": "text"
      },
      "source": [
        "For a more complex example, we have, in Step 4:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJRY0ROSng44",
        "colab_type": "code",
        "outputId": "68628544-e73e-4c2b-b6bc-d77b255aa07e",
        "colab": {}
      },
      "source": [
        "word = \"adoption\"\n",
        "suffix = \"ion\"\n",
        "stem = get_stem(suffix, word)\n",
        "apply_rule(measure(stem) > 1 and (ends_with(\"s\", stem) or ends_with(\"t\", stem)), suffix, None, word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'adopt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phWz_1idng45",
        "colab_type": "text"
      },
      "source": [
        "In total, the Porter Stemmer algorithm (for the English language) applies several dozen rules (see https://tartarus.org/martin/PorterStemmer/def.txt for a complete list). Implementing all of them is both tedious and error prone, so we abstain from providing a full implementation of the algorithm here. High quality implementations can be found in all major NLP libraries such as [NLTK](http://www.nltk.org/howto/stem.html).\n",
        "\n",
        "The dificulties of defining matching rules to arbitrary text cannot be fully resolved without the use of Regular Expressions (typically implemented as Finite State Machines like our __measure__ implementation above), a more advanced topic that is beyond the scope of this course."
      ]
    }
  ]
}